{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè≥Ô∏è Your Country is INDIA üè≥Ô∏è\n",
    "##  Your Area? üö© Your City? üö© Your State? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200419083454-0000\n",
      "KERNEL_ID = b6af1528-8ae1-4c44-aa13-887147143283\n"
     ]
    }
   ],
   "source": [
    "# Enter your Info\n",
    "State= 'karnataka'\n",
    "City= 'bangalore'\n",
    "Area= 'Agram'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/spark/shared/conda/envs/python3.6\n",
      "\n",
      "  added / updated specs:\n",
      "    - geopy\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    ca-certificates-2020.4.5.1 |       hecc5488_0         146 KB  conda-forge\n",
      "    certifi-2020.4.5.1         |   py36h9f0ad1d_0         151 KB  conda-forge\n",
      "    geographiclib-1.50         |             py_0          34 KB  conda-forge\n",
      "    geopy-1.21.0               |             py_0          58 KB  conda-forge\n",
      "    openssl-1.1.1f             |       h516909a_0         2.1 MB  conda-forge\n",
      "    python_abi-3.6             |          1_cp36m           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         2.5 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  geographiclib      conda-forge/noarch::geographiclib-1.50-py_0\n",
      "  geopy              conda-forge/noarch::geopy-1.21.0-py_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.6-1_cp36m\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    pkgs/main::ca-certificates-2019.5.15-1 --> conda-forge::ca-certificates-2020.4.5.1-hecc5488_0\n",
      "  certifi               pkgs/main::certifi-2019.6.16-py36_1 --> conda-forge::certifi-2020.4.5.1-py36h9f0ad1d_0\n",
      "  openssl              pkgs/main::openssl-1.1.1d-h7b6447c_0 --> conda-forge::openssl-1.1.1f-h516909a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "ca-certificates-2020 | 146 KB    | ##################################### | 100% \n",
      "openssl-1.1.1f       | 2.1 MB    | ##################################### | 100% \n",
      "certifi-2020.4.5.1   | 151 KB    | ##################################### | 100% \n",
      "geopy-1.21.0         | 58 KB     | ##################################### | 100% \n",
      "geographiclib-1.50   | 34 KB     | ##################################### | 100% \n",
      "python_abi-3.6       | 4 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting lxml\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/ba/a0e6866057fc0bbd17192925c1d63a3b85cf522965de9bc02364d08e5b84/lxml-4.5.0-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
      "\u001b[K    100% |################################| 5.8MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-4.5.0\n",
      "Info dropped @ Jayangar Iii block\n",
      "Venue Info dropped @ Bangalore Dist offices bldg\n",
      "Venue Info dropped @ Chikkalasandra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/ibm/conda/miniconda3.6/lib/python3.6/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\t\t\t\t‚ù§‚ù§Agram‚ù§‚ù§\n",
      "\n",
      "Domino's Pizza ‚òÖ Khan Saheb ‚òÖ Infinitea ‚òÖ Ooty Chocolates ‚òÖ Tandoori Taal ‚òÖ The Kitchen Of Joy ‚òÖ California Burrito ‚òÖ Retro Lounge Bar ‚òÖ Corner House ‚òÖ Vietnamese Kitchen ‚òÖ Namdhari's Fresh ‚òÖ Kim Lee ‚òÖ Annapoorani ‚òÖ Mamagoto ‚òÖ Phobidden Fruit ‚òÖ Imperial Restaurant ‚òÖ Lavonne ‚òÖ Mother Cluckers ‚òÖ Chianti Ristorante & Wine Bar ‚òÖ Krispy Kreme Doughnuts ‚òÖ High Note Bar and Dining ‚òÖ Bricklane Grill ‚òÖ Vapour - Pub and Brewery ‚òÖ Chaipatty Teafe ‚òÖ Smoor Chocolates by Bliss ‚òÖ 1131 Bar N Kitchen ‚òÖ Bhartiya Jalpan ‚òÖ Bob's Bar ‚òÖ Bite Me Cupcakes ‚òÖ Loft38 Lounge ‚òÖ Cream Centre ‚òÖ Barbeque Nation (Lido Mall) ‚òÖ Hyatt Centric Mg Road Bangalore ‚òÖ flying spaghetti monster ‚òÖ Nagarjuna ‚òÖ Bangalore Mandarin ‚òÖ Little Italy ‚òÖ Starbucks ‚òÖ Domino's Pizza ‚òÖ Bliss Chocolate Lounge ‚òÖ Sri Udupi Park ‚òÖ Drops Total Spirits ‚òÖ The Ants Cafe ‚òÖ Harry's Singapore ‚òÖ Rajdhani ‚òÖ Hotel Shalimar Bar and Restaurant ‚òÖ The Tao Terraces ‚òÖ Esplanade Calcutta Cuisine ‚òÖ Creme & Crust ‚òÖ Meghana Foods ‚òÖ Yauatcha ‚òÖ H√§agen-Dazs ‚òÖ The Coffee Bean & Tea Leaf ‚òÖ Cafe 42 ‚òÖ Vivanta by Taj ‚òÖ Breadworks ‚òÖ Barbeque Nation ‚òÖ Ice Bar ‚òÖ Graze ‚òÖ The People's Art Cafe ‚òÖ Mainland China ‚òÖ Conrad Bengaluru ‚òÖ Cafe Mozaic ‚òÖ Monsoon @ The Park ‚òÖ The Grill House ‚òÖ Carnival-De-Goa ‚òÖ Stoner ‚òÖ i-BAR ‚òÖ Haagen Dazs ‚òÖ Starbucks ‚òÖ Marzipan Cafe & Bakery ‚òÖ Mama Mia Gelato ‚òÖ Terra ‚òÖ Natural Ice Cream ‚òÖ The Oberoi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "\n",
    "# insall lxml to read an html file\n",
    "!pip install lxml\n",
    "\n",
    "# Read the html file\n",
    "df = pd.read_html(r'https://www.mapsofindia.com/pincode/india/{}/{}/'.format(State, City), header = 1)\n",
    "df = pd.DataFrame(df[0])\n",
    "df =df.sort_values(by='Location', ascending=True)\n",
    "# df= df.drop_duplicates(subset=\"Pincode\", keep=\"last\")\n",
    "\n",
    "# Read address for each locality\n",
    "Stat= df.iloc[0,2]\n",
    "list=[]\n",
    "for loc, pin, dis in zip(df['Location'], df['Pincode'], df['District']):\n",
    "    try:\n",
    "# Use nested try/ Except to handle the error[] alternatively and finally pass to final 'Except'\n",
    "        try:\n",
    "            address = '{}, {}'.format(pin, Stat)\n",
    "            geolocator = Nominatim(user_agent=\"Ind_Explorer\", timeout = 10)\n",
    "            location = geolocator.geocode(address)\n",
    "            lat = location.latitude\n",
    "            lng = location.longitude\n",
    "            list.append([(\n",
    "                loc,\n",
    "                pin,\n",
    "                dis,\n",
    "                lat,\n",
    "                lng)])\n",
    "    \n",
    "        except:\n",
    "            address = '{}, {}'.format(loc, Stat)\n",
    "            geolocator = Nominatim(user_agent=\"BLR_explorer\", timeout = 10)\n",
    "            location = geolocator.geocode(address)\n",
    "            lat = location.latitude\n",
    "            lng = location.longitude\n",
    "            list.append([(\n",
    "                loc,\n",
    "                pin,\n",
    "                dis,\n",
    "                lat,\n",
    "                lng)])\n",
    "    except:\n",
    "        print('Info dropped @ {}'.format(loc))\n",
    "\n",
    "fil_df = pd.DataFrame([item for venue in list for item in venue])\n",
    "fil_df.columns = ['Location', \n",
    "                    'Pincode',\n",
    "                    'District', \n",
    "                    'Latitude',\n",
    "                    'Longitude']\n",
    "\n",
    "# read client information\n",
    "CLIENT_ID = 'T3MED4EITMTO2OCQXNLAPZPTXP3T0LIOONZ4FI2LYIG2ZU0H' # your Foursquare ID\n",
    "CLIENT_SECRET = 'N2GKGR134RH1GAFSG4WPSB1EFRCJYOYCWED5CPGZZC3KDWRM' # your Foursquare Secret\n",
    "VERSION = '20180604'\n",
    "\n",
    "# Read the venues from Foursquare API\n",
    "venues_list=[]\n",
    "radius=1500\n",
    "LIMIT =100\n",
    "\n",
    "for loc, dis, lat, lng in zip(fil_df['Location'], fil_df['District'], fil_df['Latitude'], fil_df['Longitude']):\n",
    "    \n",
    "        # create the API request URL\n",
    "    try:\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'.format(\n",
    "            CLIENT_ID, \n",
    "            CLIENT_SECRET, \n",
    "            VERSION, \n",
    "            lat, \n",
    "            lng, \n",
    "            radius, \n",
    "            LIMIT)\n",
    "            \n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()['response']['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            loc, \n",
    "            dis,  \n",
    "            x['venue']['name'], \n",
    "            x['venue']['categories'][0]['name'],\n",
    "            x['venue']['location']['distance']) for x in results])\n",
    "    except:\n",
    "        print('Venue Info dropped @ {}'.format(loc))\n",
    "    \n",
    "venue_df = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "venue_df.columns = ['Location', \n",
    "                    'District', \n",
    "                    'Landmark', \n",
    "                    'Category',\n",
    "                    'Distance']\n",
    "venue_df =venue_df.sort_values(by='Location', ascending=True)\n",
    "venue_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# filter the food Dataframe\n",
    "food_df= venue_df[venue_df['Category'].str.contains('Restaurant|Milk|Sweet|lunch|Kabab|Egg|Smoke|Pizza|Joint|Dim sum|Coffee|Smoke|Sweet shop|Caf√©|Bistro|Hotel|Bar|Ice cream|Sandwich|Steak|BBQ|Liquor|Brewery|Joint|Burger|Candy|Tea|Cake|Bakery|Burrito|Snack|Bistro|Food|Breakfast|Juice|Dessert|Snack|Beer|Wine|Donut|Fast food|Salad|Diner|Fried|Chicken|Drink|Hookah|Chocolate', regex=True, case=False)]\n",
    "food_df.reset_index(drop=True, inplace = True)\n",
    "food_df.head()\n",
    "\n",
    "# drop column, 'Category'\n",
    "food_df.drop(['Category'], axis=1, inplace=True)\n",
    "\n",
    "# segregate all unique locations\n",
    "un_loc=food_df['Location'].unique()\n",
    "\n",
    "# sort the subdataframes by distance, for each location\n",
    "final_df=pd.DataFrame(columns=['Location', 'District', 'Landmark', 'Distance'])\n",
    "for loc,lc in zip(food_df['Location'], un_loc):\n",
    "    sbs = food_df[food_df['Location'] == lc]\n",
    "    sbs =sbs.sort_values(by='Distance', ascending=True)\n",
    "    final_df= final_df.append(sbs)\n",
    "    \n",
    "# drop column, 'Distance'\n",
    "final_df.reset_index(drop=True, inplace = True)\n",
    "final_df.drop(['Distance'], axis=1, inplace=True)\n",
    "\n",
    "# Merge the venues for similar locations into a dictionary\n",
    "lnd= final_df.groupby('Location')['Landmark'].agg(' ‚òÖ '.join)\n",
    "\n",
    "# Drop duplicate location rows\n",
    "final_df= final_df.drop_duplicates(subset=\"Location\", keep=\"first\")\n",
    "final_df.reset_index(drop=True, inplace = True)\n",
    "\n",
    "# insert collection of venues to the specified location in Dataframe\n",
    "for i,j in zip(final_df['Landmark'], lnd):\n",
    "    final_df.replace(i,j, inplace=True)\n",
    "    \n",
    "# drop column, 'District'\n",
    "final_df.drop(['District'], axis=1, inplace=True)\n",
    "\n",
    "# Export dataframe to csv file \n",
    "name= 'Fake_Zomato'\n",
    "export_csv = final_df.to_csv ('{}.csv'.format(name), index = None, header=True)\n",
    "\n",
    "# use get_close_matches to find area of interest and print final Info\n",
    "import difflib\n",
    "from difflib import get_close_matches\n",
    "\n",
    "lib = difflib.get_close_matches(Area, final_df['Location'])\n",
    "\n",
    "for i,j in zip(final_df['Location'], final_df['Landmark']):\n",
    "    if i==lib[0]:\n",
    "        print('\\n\\t\\t\\t\\t\\t‚ù§‚ù§{}‚ù§‚ù§\\n\\n{}\\n'.format(i,j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
